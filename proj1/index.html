<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Rasterizer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Fall 2023</h1>
<h1 align="middle">Project 1: Rasterizer</h1>

<h2 align="middle">Alex Liang-Duan Lyu, CS184-Assignment-1</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>In this project I built a Rasterizer in C++ that can render files using a modified version of the SVG (Scalable Vector Graphics) format. It implements features such as drawing triangles, antialiasing through supersampling, matrix transforms, and texture mapping through both “Pixel Sampling” and “Level Sampling”. Through completing this project I have learned how images are represented as data, how the SVG format works and its differences from popular formats like JPEG, how to implement the different mathematical formulas shown in lectures via code, and certain functionalities that the OpenGL library provides. I also got to see the real visual effects that different antialiasing methods have on renders with varying issues, which I thought was very cool!</p>
<h2 align="middle">Section I: Rasterization</h2>

<h3 align="middle">Part 1: Rasterizing single-color triangles</h3>
<p>To rasterize a triangle, I first computed its bounding box by calculating the maximum and minimum x and y values among its 3 vertices, values that I assigned to the variables xmax, xmin, ymax, and ymin. Then I literated through every point within the bounding box, checking if they are in the triangle using the point-in-triangle test outlined in lecture 2. If a point passes all 3 line tests (given by the helper function in_line), then the pixel is filled in. </p>
<p>My algorithm employs 2 nested for loops bounded by the variables xmax, xmin, ymax, and ymin, so it checks each sample point inside the bounding box exactly once, and none outside the box. It is the exact same as the algorithm mentioned in the rubric, therefore obviously no worse than it.</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task1.png" align="middle" width="400px"/>
        <figcaption align="middle">basic/test4.svg</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 2: Antialiasing triangles</h3>

<p>My supersampling algorithm first samples the image at a higher resolution (each pixel is split into a grid of “subpixels” and sampled sqrt(sample_rate) * sqrt(sample_rate) times), and then stores the data to the RasterizerImp::sample_buffer vector, which is resized dynamically according to the size of the frame buffer and the sample rate. The sample buffer is then downsampled via an average of color values for the “subpixels” to populate the frame buffer. When the downsampling is finished, the program renders the supersampled image using the frame buffer. </p>
<p>So now when supersampling is enabled, we essentially add an extra step to the rasterization pipeline: instead of sampled color values going straight to the frame buffer, they go to the sample buffer first, then goes through RasterizerImp::resolve_to_framebuffer() to get resolved to the frame buffer. I also adjusted the number of samples in rasterize_triangle to be determined by the given sample rate, added an argument to the fill_pixel method so that supersampling won’t be applied to lines and points, and changed the frame buffer population process so that it now obtains pixels from the higher resolution image. </p>
<p>Supersampling is useful because it reduces aliasing artifacts such as jaggies. Instead of a pixel being “all or nothing” (i.e. either rendered or blank), supersampling allows for pixels with color values in between those two based on the proportion of pixels that is inside the triangle, which makes the image more visually appealing, and easier for people to perceive the edge as “smooth”. </p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/rate1.png" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate = 1</figcaption>
      </td>
      <td>
        <img src="images/rate2.png" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate = 4</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/rate3.png" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate = 16</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>When comparing the images listed above, we can observe that as sample rate increases, some of the previously blank pixels become slightly more colored, and some of the previously filled in pixels become slightly less colored. This is because as sample rate increases, more sample points within a single pixel on the screen are sampled, which allows for some pixels that were partially inside the triangle, but left blank because their centers weren’t inside, to be rendered because some of their “subpixels” will become colored which will increase the average. Similarly, some pixels that were partially inside the triangle but colored because their centers were inside will have some of their “subpixels” turn blank which brings the average down and leads to lighter color. </p>

<p>Apart from grid sampling, I also tried a different sampling method, jittered sampling. In this method, instead of sampling the center of each grid, the program samples at a random position with the pixel. The position is obtained through the equation (x = x + (rand() % (2 * scale)) * factor) where sx is the x-coordinate of the upper-left corner of the pixel, scale is sqrt(sample_rate), and factor is equal to grid_width/2. With this method, the rendering result is actually more visually appealing. As you can see from the demonstration below, when the sample rate is the same, jittered sampling sampled more pixels at the upper vertex of the pink triangle. This created a smoother, better_connected triangle edge because it led to less white pixels. </p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/jittered.png" align="middle" width="400px"/>
        <figcaption align="middle">Jittered Sampling</figcaption>
      </td>
      <td>
        <img src="images/no_jittered.png" align="middle" width="400px"/>
        <figcaption align="middle">Grid Sampling</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 3: Transforms</h3>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/my_robot.png" align="middle" width="400px"/>
        <figcaption align="middle">My Robot</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>I rotated the torso and the arm of the robot, and also translated its head and arm. I wanted to make it look like Gru in the movie Despicable Me waving his hand. </p>

<h2 align="middle">Section II: Sampling</h2>

<h3 align="middle">Part 4: Barycentric coordinates</h3>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/color_triangle.png" align="middle" width="400px"/>
        <figcaption align="middle">Color Triangle</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Barycentric coordinates is a way to describe a point in a triangle using a weighted sum of the three vertices that define the triangle. For a triangle defined by vertices A, B, and C, a point P will have the coordinates (a, b, c), where coord(P) = a*coord(A) + b*coord(B) + c*coord(C), and a+b+c=1. When P is closer to a vertex, the corresponding weight for that vertex will be higher, and when P equals said vertex the weight will become 1. This idea can be generalized to many other properties of the three vertices. In the image shown above, the RGB value of each pixel is determined by its barycentric coordinates that correspond to the red, green, and blue vertices respectively. We can see that the closer to a vertex a pixel is, the more similar its color will be to that vertex. </p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/basic7.png" align="middle" width="400px"/>
        <figcaption align="middle">basic/test7.svg</figcaption>
      </td>
    </tr>
  </table>
</div>
<h3 align="middle">Part 5: "Pixel sampling" for texture mapping</h3>

<p>Pixel sampling is essentially the process of selecting a specific RGB value for a pixel in an image that will be displayed on a screen by sampling from its corresponding coordinates in a file. So far in this project, we have been sampling from a svg file passed to the program, but we can also implement texture mapping via pixel sampling by taking values not from an svg file, but from an image that contains the texture of the object that we wish to render. For each coordinate of the target object (called “screen space”, can be 3D or 2D), we map it to a corresponding coordinate in the 2D texture image (called “texture space”), and then we proceed with our sampling. My implementation of “pixel sampling” consists of two methods: for nearest neighbor sampling, my algorithm computes the Euclidean distance between the given sample point and the texels that surround it, ignoring out-of-bounds texels if the sample point is at the corner. Then it simply picks the color of the texel that has the smallest Euclidean distance to be the color of the pixel in screen space; for bilinear sampling, my algorithm will return a weighted average of the colors of the four texel vertices next to the sample point based on their relative positions with respect to the sample point. If the sample point is at a corner, the algorithm will use nearest neighbor sampling instead. </p>

<p>Nearest neighbor sampling is a very easy way to perform texture mapping because it simply takes the color of the closest texel to the mapped point of our target pixel. This may produce jaggies and aliasing, especially when the image being rendered is magnified in screen space and thus requires upsampling. In that case the footprint of the pixel in texture space can become so small that individual pixels could be relatively far from the center of the texel that it maps to, and multiple pixels could map to the exact same single texel, making it less than ideal to simply pick the color of a nearest neighbor. Bilinear sampling fixes this issue and other aliasing artifacts via a process of interpolation that computes the color value of a pixel using its surrounding texels. It produces a weighted average that only takes into account the position of the pixel, regardless of how small its footprint in texture space is. It can also differentiate different pixels that would have otherwise been mapped to the exact same texel. And of course, because it takes into account at least 4 texels for any given pixel, it can achieve an antialiasing effect similar to supersampling which smoothes out jaggies and makes the image more visually appealing. In the images below, we observe that near the middle where screen space became more magnified, bilinear sampling takes a very noticeable advantage over nearest neighbor sampling, and in fact we can even observe that bilinear sampling with a sampling rate of 1 can achieve a similar effect to nearest neighbor sampling with a sampling rate of 16.</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/1_nearest.png" align="middle" width="400px"/>
        <figcaption align="middle">Nearest neighbor with sample rate = 1</figcaption>
      </td>
      <td>
        <img src="images/1_linear.png" align="middle" width="400px"/>
        <figcaption align="middle">Bilinear with sample rate = 1</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/16_nearest.png" align="middle" width="400px"/>
        <figcaption align="middle">Nearest neighbor with sample rate = 16</figcaption>
      </td>
      <td>
        <img src="images/16_linear.png" align="middle" width="400px"/>
        <figcaption align="middle">Bilinear with sample rate = 16</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 6: "Level sampling" with mipmaps for texture mapping</h3>

<p>Level sampling, also known as mipmapping, is the technique of texture mapping that aims to improve the output’s visual quality and the performance/efficiency of the mapping algorithm, especially in the case of texture minification. It precomputes a series of texture images with decreasing resolution, so that the algorithm can select the appropriate one to sample from at runtime, in order to reduce computational cost (number of samples) and minimize aliasing artifacts for parts of the image that are far away/zoomed out and thus have a larger pixel footprint in texture space. </p>

<p>I implemented two different mipmapping methods, nearest level and linear, in this assignment. For the nearest level method, the program first computes the ideal mipmapping level of the sample point using derivatives of the barycentric coordinates of 3 different points with respect to their positions in the screen space. Then, it rounds the level, which is originally a floating point number, to the nearest integer level, and uses the texture from that level for texture sampling. The sampling process is the same as the one described in task 5. For invalid levels(out-of-bounds), I will clamp it to the nearest valid level. For linear mipmapping, I will first compute the level using the same method as before. Then, the program will compute the distance from the raw get_level() result to the levels that are higher and lower than it. Next, the program samples from both the higher level and lower level to get two color values. The previously calculated distances will then be used to weigh the two color values to compute the final color value to be returned. For the edge case where the raw get_level() result is not bounded by two valid levels, the program will then use the result of nearest level mipmapping instead.  </p>

<p>With all the previous techniques implemented, I can now render an image using 6 different texture mapping methods, as well as supersampling. However, there are trade offs between these methods. First of all, increasing the number of samples per pixel increases the antialiasing power and produces higher quality images, but it exponentially increases the runtime and memory used for the sample buffer. For pixel sampling techniques, the nearest neighbor sampling is fast but may produce images with jaggies. The bilinear sampling produces images with smoother edges so that they are more visually appealing and helps during texture magnification, but it increases the runtime and computational costs. The bilinear sampling has slightly higher usage of runtime stack memory because it needs more local variables for computation than the nearest neighbor method. But that overhead is not as important as the extra cost of rendering speed. For level sampling, the nearest level is faster than the bilinear method but produces less appealing images. It uses less memory for local variables for computation at runtime. But these two methods in general do not lead to increase in size of sample buffer. They often produce better images than simply sampling from the highest resolution texture. So the bottleneck is again the rendering speed. In general, supersampling is a good way to reduce aliasing for rasterization, but it is probably too costly for the benefits that it brings when it comes to texture mapping. Instead, bilinear filtering should be used when the picture gets magnified, and level sampling/trilinear filter should be used when there exists parts of the texture map that are especially minified. </p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/l_0&p_linear.png" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_LINEAR</figcaption>
      </td>
      <td>
        <img src="images/l_0&p_nearest.png" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_NEAREST</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/l_nearest&p_linear.png" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_LINEAR</figcaption>
      </td>
      <td>
        <img src="images/l_nearest&p_nearest.png" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_NEAREST</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/l_0&p_linear_zoom.png" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_LINEAR zoomed</figcaption>
      </td>
      <td>
        <img src="images/l_0&p_nearest_zoom.png" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_NEAREST zoomed</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/l_nearest&p_linear_zoom.png" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_LINEAR zoomed</figcaption>
      </td>
      <td>
        <img src="images/l_nearest&p_nearest_zoom.png" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_NEAREST zoomed</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>If you take a closer look at areas at the upper left corner, you can see the differences I mentioned before. L_ZERO_P_NEAREST apparently has more moires at the area that is further away than L_ZERO_P_LINEAR does. L_ZERO_P_NEAREST has more moires at the same area than L_NEAREST_P_NEAREST does. On the other hand, L_ZERO_P_LINEAR clearly has smoother lines and less moires compared to L_ZERO_P_NEAREST. L_NEAREST_P_LINEAR clearly has smoother lines and less moires compared to L_NEAREST_P_NEAREST. L_ZERO_P_LINEAR and L_NEAREST_P_LINEAR is very similar in terms of moires. In general, when using bilinear texture sampling, the image has less jaggies. When using the nearest level sampling, the image tends to have less moires in the area that are further away. But using with bilinear sampling, the improvement is subtle, as shown by L_ZERO_P_LINEAR and L_NEAREST_P_LINEAR. </p>

<h2 align="middle">Section III: Art Competition</h2>

<h3 align="middle">Part 7: Draw something interesting!</h3>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/competition.png" align="middle" width="400px"/>
        <figcaption align="middle">The Milky Way</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>I used an image of a galaxy as a texture image and rendered a seal-like object. It created this spiral like image that kind of looks like the Milky Way!</p>

</body>
</html>
